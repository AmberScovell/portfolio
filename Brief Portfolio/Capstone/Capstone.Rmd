---
title: "Capstone"
author: "Merlin Star"
date: "5/25/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'hide', eval=FALSE)

```

# Code Introduction

I want to examine the dimensions of violence and mental health in adolescents. 

```{r Intro}
#Libraries needed
library("covLCA") #For latent class analysis
library("CTT") #For Classical Test Theory Item Analysis
library("depmixS4")
library("dplyr")
library("ggparallel")
library("ggplot2") #For plotting
library("knitr")
library("Lambda4")
library("ltm")
library("igraph")
library("itemanalysis")
library("MASS")
library("magrittr") #For %<>% function
library("MBESS")
library("mpath")
library("plyr") #For latent class analysis
library("poLCA")
library("psych")
library("psychometric") #for CTT item analysis
library("readxl") #For importing the dataset
library("reshape2")
library("survey") #For analysis
library("tidyr")

#Dataset and Randomizer
fulldata = read_xlsx("XXHq.xlsx")
set.seed(2564505)
reduceddata = fulldata[,18:35] #These are questions 12-29. 12-22 are related to violence, 23 and 24 are bullying related, and 25-29 are mental health related.

#Converting from character to numeric for the relevant sections
reduceddata$q12 = as.numeric(reduceddata$q12)
reduceddata$q13 = as.numeric(reduceddata$q13)
reduceddata$q14 = as.numeric(reduceddata$q14)
reduceddata$q15 = as.numeric(reduceddata$q15)
reduceddata$q16 = as.numeric(reduceddata$q16)
reduceddata$q17 = as.numeric(reduceddata$q17)
reduceddata$q18 = as.numeric(reduceddata$q18)
reduceddata$q19 = as.numeric(reduceddata$q19)
reduceddata$q20 = as.numeric(reduceddata$q20)
reduceddata$q21 = as.numeric(reduceddata$q21)
reduceddata$q22 = as.numeric(reduceddata$q22)
reduceddata$q23 = as.numeric(reduceddata$q23)
reduceddata$q24 = as.numeric(reduceddata$q24)
reduceddata$q25 = as.numeric(reduceddata$q25)
reduceddata$q26 = as.numeric(reduceddata$q26)
reduceddata$q27 = as.numeric(reduceddata$q27)
reduceddata$q28 = as.numeric(reduceddata$q28)
reduceddata$q29 = as.numeric(reduceddata$q29)

#Converting to factor for the relevant sections
FactoredQ12 = factor(reduceddata$q12)
FactoredQ13 = factor(reduceddata$q13)
FactoredQ14 = factor(reduceddata$q14)
FactoredQ15 = factor(reduceddata$q15)
FactoredQ16 = factor(reduceddata$q16)
FactoredQ17 = factor(reduceddata$q17)
FactoredQ18 = factor(reduceddata$q18)
FactoredQ19 = factor(reduceddata$q19)
FactoredQ20 = factor(reduceddata$q20)
FactoredQ21 = factor(reduceddata$q21)
FactoredQ22 = factor(reduceddata$q22)
FactoredQ23 = factor(reduceddata$q23)
FactoredQ24 = factor(reduceddata$q24)
FactoredQ25 = factor(reduceddata$q25)
FactoredQ26 = factor(reduceddata$q26)
FactoredQ27 = factor(reduceddata$q27)
FactoredQ28 = factor(reduceddata$q28)
FactoredQ29 = factor(reduceddata$q29)

FactoredData = cbind(FactoredQ12,FactoredQ13,FactoredQ14,FactoredQ15,FactoredQ16,FactoredQ17,FactoredQ18,FactoredQ19,FactoredQ20,FactoredQ21,FactoredQ22,FactoredQ23,FactoredQ24,FactoredQ25,FactoredQ26,FactoredQ27,FactoredQ28,FactoredQ29) #Data in factored form for sections 2 and 3

NoMissingData = na.omit(reduceddata) #From 13,677 to 6,788! SO MUCH MISSING DATA
NoMissingFactoredData = na.omit(FactoredData)

#Converting answers for questions 21 and 22 for section 2
NoMissingData %<>%
  mutate(ReducedFactorQ21=case_when(
    NoMissingData$q21 %in% 1:2 ~ 1,
    NoMissingData$q21 %in% 3 ~ 2,
    NoMissingData$q21 %in% 4 ~ 3,
    NoMissingData$q21 %in% 5 ~ 4,
    NoMissingData$q21 %in% 6 ~ 5
  )) #This is for question 21 to collapse answers A and B into 1 and leave the others unaffected.
NoMissingData %<>%
  mutate(ReducedFactorQ22=case_when(
    NoMissingData$q22 %in% 1:2 ~ 1,
    NoMissingData$q22 %in% 3 ~ 2,
    NoMissingData$q22 %in% 4 ~ 3,
    NoMissingData$q22 %in% 5 ~ 4,
    NoMissingData$q22 %in% 6 ~ 5
  )) #This is for question 22 to collapse answers A and B into 1 and leave the others unaffected.


#Creating a key for Section 2
key12 = ifelse(NoMissingData$q12==1, 1, 0) #If question 12 is 1, set it to 1, Otherwise, set it to 0.
key13 = ifelse(NoMissingData$q13==1, 1, 0) #So on and so forth
key14 = ifelse(NoMissingData$q14==1, 1, 0) 
key15 = ifelse(NoMissingData$q15==1, 1, 0) 
key16 = ifelse(NoMissingData$q16==1, 1, 0) 
key17 = ifelse(NoMissingData$q17==1, 1, 0) 
key18 = ifelse(NoMissingData$q18==1, 1, 0) 
key19 = ifelse(NoMissingData$q19==2, 1, 0) #Make sure to change the first number to 2 depending on the question!
key20 = ifelse(NoMissingData$q20==1, 1, 0) 
key21 = ifelse(NoMissingData$ReducedFactorQ21==1, 1, 0) 
key22 = ifelse(NoMissingData$ReducedFactorQ22==1, 1, 0)
key23 = ifelse(NoMissingData$q23==2, 1, 0) 
key24 = ifelse(NoMissingData$q24==2, 1, 0) 
key25 = ifelse(NoMissingData$q25==2, 1, 0) 
key26 = ifelse(NoMissingData$q26==2, 1, 0) 
key27 = ifelse(NoMissingData$q27==2, 1, 0) 
key28 = ifelse(NoMissingData$q28==1, 1, 0) 
key29 = ifelse(NoMissingData$q29==1, 1, 0)

key = key12+key13+key14+key15+key16+key17+key18+key19+key20+key21+key22+key23+key24+key25+key26+key27+key28+key29
```

```{r Part1}
attach(reduceddata) #For ease of executing functions in Section 1


#Latent Class Analysis (Section 1)

lca.polca4 = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22,q23,q24,q25,q26,q27,q28,q29)~1,nclass=4,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000)# 4 classes will be the initial amount assumed since 4 classes are likely to be in there (bullying, weapon carrying, dating violence, and suicide). In addition, the model will be estimated multiple times for the more global result. Missing values will be included since the participants were allowed to answer and skip questions as they pleased. 

lca.polca5 = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22,q23,q24,q25,q26,q27,q28,q29)~1,nclass=5,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #Testing with 5 classes
lca.polca3 = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22,q23,q24,q25,q26,q27,q28,q29)~1,nclass=3,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #Testing with 3 classes
lca.polca2 = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22,q23,q24,q25,q26,q27,q28,q29)~1,nclass=2,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #Just 2 classes
lca.polca6 = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22,q23,q24,q25,q26,q27,q28,q29)~1,nclass=6,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #Testing with 6 classes

#From this, 3 classes should be used.

subset1.lca = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22)~1,
                    nclass=2,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #2 classes
subset2.lca = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22)~1,
                    nclass=3,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #3 classes
subset3.lca = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22)~1,
                    nclass=1,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #1 class
subset4.lca = poLCA(cbind(q12,q13,q14,q15,q16,q17,q18,q19,q20,q21,q22)~1,
                    nclass=4,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #4 classes
#2 classes is best for questions 12-22

#Questions 23 and 24 are too small, so group in with questions 25-29

subset5.lca = poLCA(cbind(q23,q24,q25,q26,q27,q28,q29)~1,
                    nclass=3,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #3 classes
subset6.lca = poLCA(cbind(q23,q24,q25,q26,q27,q28,q29)~1,
                    nclass=2,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #2 classes
subset7.lca = poLCA(cbind(q23,q24,q25,q26,q27,q28,q29)~1,
                    nclass=1,data=reduceddata,nrep=3,na.rm=F,graphs=T,maxiter = 50000) #1 class
#2 classes for questions 23-29 is best

detach(reduceddata)
```


```{r Part2}

#Classical Test Theory Item Analysis (Section 2)

#Had to reformat Questions 21 and 22 for this specific section since A or B were desireable answers.
#For question 29, A and C were both desireable answers, but clearly A was more desireable than C.

item.exam(x=NoMissingData[,c(1:9, 19, 20, 12:18)],y = key, discrim = T)


# Item Response Theory (Section 3)
#Divided into 12-22 and 23-29.

rcor.test(NoMissingData[,c(1:18)])
descript(NoMissingData[,c(1:11)]) #Question 19 was flagged as decreasing alpha
descript(NoMissingData[,c(1:7,9:11)])
rcor.test(NoMissingData[,c(1:11)])
grm1 =grm(NoMissingData[,c(1:11)], constrained = F)
grm2 =grm(NoMissingData[,c(1:11)], constrained = T)
summary.grm(grm1)
summary.grm(grm2)
coef.grm(grm1)
coef.grm(grm2)
margins.grm(grm1)
margins.grm(grm2) #Constraining it is better but still poor
plot.grm(grm1)
plot.grm(grm2)

descript(NoMissingData[,c(12:18)]) #The last 2 questions are responsible for the dramatic dip in alpha. Good to know those are distinct.
rcor.test(NoMissingData[,c(12:18)])
grm3 =grm(NoMissingData[,c(12:18)], constrained = F)
grm4 =grm(NoMissingData[,c(12:18)], constrained = T)
summary.grm(grm3)
summary.grm(grm4)
coef.grm(grm3)
coef.grm(grm4)
margins.grm(grm3)
margins.grm(grm4) #Fit is extremely poor with both, but constraining it made the fit even worse
plot.grm(grm3)
plot.grm(grm4)


#These are questions 12-29. 12-22 are related to violence, 23 and 24 are bullying related, and 25-29 are mental health related.

```

```{r Part3}
#Validity (Section 4)
attach(reduceddata)
data.factor<-factanal(~q12+q13+q14+q15+q16+q17+q18+q19+q20+q21+q22+q23+q24+q25+q26+q27+q28+q29, factors=3, rotation='none')
data.factor
data.factor2 = factanal(~q12+q13+q14+q15+q16+q17+q18+q19+q20+q21+q22+q23+q24+q25+q26+q27+q28+q29, factors=4, rotation='none')
data.factor2
data.factor3 = factanal(~q12+q13+q14+q15+q16+q17+q18+q19+q20+q21+q22+q23+q24+q25+q26+q27+q28+q29, factors=5, rotation='none')
data.factor3 #No more than 4 factors without rotation

fa(reduceddata, nfactors=3, residuals=TRUE, rotate="promax", SMC=TRUE, fm="pa")
fa(reduceddata, nfactors=4, residuals=TRUE, rotate="promax", SMC=TRUE, fm="pa") # 3 or fewer
fa(reduceddata, nfactors=2, residuals=TRUE, rotate="promax", SMC=TRUE, fm="pa") 

#3 seems best with this rotation
fa(reduceddata, nfactors=3, residuals=TRUE, rotate="varimin", SMC=TRUE, fm="pa")
#This was the only other rotation that did not give errors. As suspected, these questions are 3 distinct domains.

data.pca = princomp(~q12+q13+q14+q15+q16+q17+q18+q19+q20+q21+q22+q23+q24+q25+q26+q27+q28+q29, fix_sign = TRUE)

data.pca$loadings
```

```{r Part4}
#Reliability (Section 5)
psych::alpha(FactoredData) #This is low (.65), however it is known that this is multidimensional and that some keys likely need to be reversed.
psych::alpha(FactoredData, check.keys=TRUE) #This is much better (.8)
psych::alpha(FactoredData[,1:10]) #.71 is in the barely acceptable range
psych::alpha(FactoredData[,1:10], check.keys=TRUE) #.74, did not change much
psych::alpha(FactoredData[,11:18], check.keys = TRUE) #.8, good

#Need to experiment with alpha here
psych::alpha(NoMissingFactoredData, check.keys = TRUE) #So, Question 19, and Questions 23-27 were reversed. Not surprised given that the 2nd option would indicate no issues where as for the other question, the 1st option indicates no issues.

#Splits halves reliability estimate

angoff(FactoredData, split.method="even.odd")  #Coefficient is .775

#Guttman reliability estimates 

splitHalf(FactoredData) #Average is .82, max is .9, min is .54 (that is concerning)

glb.fa(FactoredData)
# Questions 15 and 19 were below .3
# Questions 13, 14, and 25 were below .4
# Questions 16, 18, 23, 24, 25, 27 were below .6
# All others were above .6

omega.tot(FactoredData, factors=1) #.544 for 1 factor
omega.tot(FactoredData, factors=2) #.594 for 2 factors
omega.tot(FactoredData, factors=3) #.645 for 3 factors
# 3 factors seems to be best for omega. Diminishing results kick in after 3.

# Confidence intervals
#This is with missing data
ci.reliability(data=FactoredData, type="alpha", interval.type = "bonett") #(.695, .710)
ci.reliability(data=FactoredData, type="alpha", interval.type = "feldt") #(.695, .710)
ci.reliability(data=FactoredData, type="alpha", interval.type = "fisher") #(.693, .711)
ci.reliability(data=FactoredData, type="alpha", interval.type = "hakstian") #(.695, .710)

#This is without missing data

#BE WARNED- THE BOOTSTRAPS TAKE FOREVER TO DO!
ci.reliability(data=NoMissingFactoredData, type="alpha", interval.type = "perc", B=7000) #(.540, .640)
ci.reliability(data=NoMissingFactoredData, type="alpha", interval.type = "bca", B=7000) #Had to use 7000 because the replication MUST be greater than the data rows. The data set is 6788, so I used 7000. #(.545, .644)
ci.reliability(data=NoMissingFactoredData, type="alpha", interval.type = "adf") #(.543, .642) Missing data makes a huge difference!
```

# Summary of results

### Section 1- Overall 3 classes should be used for the data. If split into subsections, 2 classes should be used for the 2 bigger subsections

### Section 2 and 3- These methods are NOT recomended.

### Section 4- 3 classes has the highest validity for this sample.

### Section 5- Reliability is acceptable in some cases and poor in others.


